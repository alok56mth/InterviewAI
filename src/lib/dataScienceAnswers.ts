// Data Scientist Interview Answers - Part 1: Statistics & Machine Learning
export const DATASCIENCE_ANSWERS = {
    statistics: {
        easy: {
            "What is the difference between mean, median, and mode?": "Mean: Average of all values (sum/count). Median: Middle value when sorted (robust to outliers). Mode: Most frequent value. Use mean for normal distributions, median for skewed data or outliers, mode for categorical data.",
            "What is standard deviation?": "Standard deviation measures spread of data around the mean. Low SD = data clustered near mean. High SD = data spread out. Calculated as square root of variance. Used for understanding data variability and detecting outliers.",
            "What is a normal distribution?": "Normal (Gaussian) distribution is bell-shaped, symmetric around mean. Mean = median = mode. 68% data within 1 SD, 95% within 2 SD, 99.7% within 3 SD. Many natural phenomena follow this distribution.",
            "What is the difference between population and sample?": "Population: Entire group you want to study. Sample: Subset of population used for analysis. We use samples because populations are often too large. Sample statistics estimate population parameters.",
            "What is variance?": "Variance measures how far data points are from the mean. Calculated as average of squared differences from mean. Higher variance = more spread. Standard deviation is square root of variance.",
            "What is correlation and what does it measure?": "Correlation measures linear relationship between two variables. Ranges from -1 (negative) to +1 (positive). 0 means no linear relationship. Pearson for linear, Spearman for ranked data.",
            "What is the difference between correlation and causation?": "Correlation: Two variables change together. Causation: One variable causes change in another. Correlation doesn't imply causation - could be coincidence, confounding variable, or reverse causation.",
            "What is a hypothesis test?": "Hypothesis test determines if results are statistically significant. Compare null hypothesis (no effect) vs alternative hypothesis. Use p-value to decide whether to reject null. Common tests: t-test, chi-square, ANOVA.",
            "What is a null hypothesis?": "Null hypothesis (H0) states no effect or no difference exists. It's the default assumption we try to disprove. Alternative hypothesis (H1) is what we want to prove. We reject null if evidence is strong enough.",
            "What is a p-value?": "P-value is probability of getting results as extreme as observed, assuming null hypothesis is true. Low p-value (typically <0.05) suggests rejecting null hypothesis. Not the probability that null is true.",
            "What is the difference between Type I and Type II errors?": "Type I (False Positive): Rejecting true null hypothesis - saying effect exists when it doesn't. Type II (False Negative): Failing to reject false null - missing a real effect. Trade-off between the two.",
            "What is confidence interval?": "Confidence interval is range of values likely to contain true population parameter. 95% CI means if we repeated experiment 100 times, 95 intervals would contain true value. Wider CI = more uncertainty.",
            "What is the Central Limit Theorem?": "CLT states that sample means approach normal distribution as sample size increases, regardless of population distribution. Enables using normal distribution for inference. Generally n>30 is sufficient.",
        },
        medium: {
            "Explain the difference between parametric and non-parametric tests.": "Parametric: Assume specific distribution (usually normal), use parameters (mean, SD). Examples: t-test, ANOVA. Non-parametric: No distribution assumption, use ranks. Examples: Mann-Whitney, Kruskal-Wallis. Non-parametric for non-normal data or small samples.",
            "What is the difference between t-test and z-test?": "Z-test: Population SD known, large samples (n>30). T-test: Population SD unknown (use sample SD), smaller samples. T-distribution has heavier tails. As n increases, t approaches z.",
            "What is ANOVA and when would you use it?": "ANOVA (Analysis of Variance) compares means of 3+ groups. Tests if at least one group differs significantly. One-way for one factor, two-way for two factors. Use when comparing multiple groups - avoids multiple t-test problems.",
            "What is chi-square test?": "Chi-square test compares observed vs expected frequencies. Used for categorical data. Goodness of fit: one variable. Independence: two variables. Assumes expected frequency >5 in each cell.",
            "What is Bayes' theorem and how is it used?": "Bayes' theorem: P(A|B) = P(B|A) * P(A) / P(B). Updates probability based on new evidence. Used in spam filtering, medical diagnosis, machine learning classifiers. Prior probability + evidence = posterior probability.",
            "What is the difference between covariance and correlation?": "Covariance: Direction of relationship, not standardized, units depend on variables. Correlation: Standardized covariance, ranges -1 to 1, dimensionless. Correlation is easier to interpret and compare.",
            "What is statistical significance?": "Statistical significance means results unlikely due to chance alone. Typically p<0.05. Doesn't mean practical importance - effect could be small. Consider effect size alongside significance.",
            "What is the law of large numbers?": "As sample size increases, sample mean converges to population mean. Foundation for statistical inference. Explains why larger samples give more reliable estimates.",
            "What is the difference between discrete and continuous distributions?": "Discrete: Countable outcomes (integers) - Binomial, Poisson. Continuous: Infinite outcomes in range - Normal, Exponential. Discrete uses PMF (probability mass), continuous uses PDF (probability density).",
            "What is a probability distribution function (PDF)?": "PDF describes probability density for continuous variables. Area under curve = probability. Total area = 1. Height is density, not probability. Probability is area over interval.",
            "What is cumulative distribution function (CDF)?": "CDF gives probability that variable is less than or equal to value. F(x) = P(X ≤ x). Ranges from 0 to 1. Derivative of CDF is PDF. Used for calculating percentiles.",
            "What is the difference between likelihood and probability?": "Probability: Given parameters, what's chance of data? Likelihood: Given data, how likely are parameters? Same formula, different perspective. Likelihood used in MLE to estimate parameters.",
            "What is maximum likelihood estimation (MLE)?": "MLE finds parameters that maximize likelihood of observed data. Most common estimation method. Finds most likely parameter values. Used extensively in ML - logistic regression, distributions fitting.",
        },
        hard: {
            "How do you handle multiple hypothesis testing?": "Multiple testing increases false positive rate. Solutions: Bonferroni correction (divide α by number of tests), Benjamini-Hochberg (controls FDR), Holm-Bonferroni (step-down), use domain knowledge to limit tests.",
            "What is the Bonferroni correction?": "Bonferroni corrects for multiple comparisons by dividing significance level by number of tests. If α=0.05 and 10 tests, use α=0.005. Conservative - may miss true effects. Alternative: FDR control.",
            "Explain A/B testing and its statistical foundations.": "A/B testing compares two versions. Random assignment to control/treatment. Calculate test statistic, p-value, effect size. Need sufficient sample size (power analysis). Consider practical significance. Handle novelty effects, Simpson's paradox.",
            "What is power analysis in hypothesis testing?": "Power = probability of detecting true effect. Power = 1 - Type II error rate. Affected by: effect size, sample size, significance level, variance. Use to determine required sample size. Typically aim for 80% power.",
            "What is bootstrapping and when would you use it?": "Bootstrapping resamples data with replacement to estimate sampling distribution. Use when: parametric assumptions violated, small samples, complex statistics. Provides confidence intervals and standard errors.",
            "What is the difference between Bayesian and Frequentist statistics?": "Frequentist: Parameters fixed, probability = long-run frequency. Bayesian: Parameters have distributions, probability = degree of belief. Bayesian uses priors, updates with data. Different philosophical foundations.",
            "How do you detect and handle outliers statistically?": "Detection: Z-score (>3), IQR (1.5*IQR beyond quartiles), Mahalanobis distance, visualization. Handling: Remove (if errors), transform, winsorize, use robust methods. Consider domain knowledge.",
            "What is kernel density estimation?": "KDE is non-parametric way to estimate PDF. Places kernel (often Gaussian) at each data point, sums them. Bandwidth controls smoothness. Alternative to histograms. Better for continuous visualization.",
            "What is the expectation-maximization algorithm?": "EM iteratively estimates parameters with missing data. E-step: Calculate expected values. M-step: Maximize likelihood. Used in: GMMs, missing data, latent variable models. Converges to local maximum.",
            "How do you validate statistical models?": "Residual analysis, R-squared/adjusted R-squared, AIC/BIC for model comparison, cross-validation, bootstrap validation, out-of-sample testing, checking assumptions (normality, homoscedasticity).",
            "What is heteroscedasticity and how do you detect it?": "Heteroscedasticity: Non-constant variance of residuals. Violates regression assumptions. Detect: Residual plots, Breusch-Pagan test, White test. Handle: Robust standard errors, weighted least squares, transform.",
            "What is multicollinearity and how do you handle it?": "Multicollinearity: High correlation between predictors. Inflates standard errors, unstable coefficients. Detect: VIF>10, correlation matrix. Handle: Remove variables, PCA, regularization (Ridge).",
            "What is the difference between homoscedasticity and heteroscedasticity?": "Homoscedasticity: Constant variance of residuals (assumption of OLS). Heteroscedasticity: Changing variance. Visualization: Residual plot should show random scatter (homo) not funnel shape (hetero).",
            "How do you perform time series analysis?": "Components: Trend, seasonality, cycles, noise. Stationarity testing (ADF), decomposition, ACF/PACF analysis, ARIMA modeling, forecasting, cross-validation (walk-forward). Handle autocorrelation.",
        }
    },
    machineLearning: {
        easy: {
            "What is machine learning?": "ML is subset of AI where systems learn patterns from data without being explicitly programmed. Types: Supervised (labeled data), Unsupervised (find patterns), Reinforcement (learn from rewards). Goal: Make predictions or decisions.",
            "What is the difference between supervised and unsupervised learning?": "Supervised: Labeled data, predict outcomes (classification, regression). Examples: spam detection, price prediction. Unsupervised: No labels, find patterns. Examples: clustering, dimensionality reduction, anomaly detection.",
            "What is a training set and test set?": "Training set: Data used to train model. Test set: Held-out data for final evaluation. Typically 70-80% train, 20-30% test. Never use test set during training. Often add validation set for hyperparameter tuning.",
            "What is overfitting?": "Overfitting: Model memorizes training data, fails on new data. Signs: High training accuracy, low test accuracy. Causes: Complex model, insufficient data. Solutions: Regularization, more data, simpler model, cross-validation.",
            "What is underfitting?": "Underfitting: Model too simple to capture patterns. Signs: Low accuracy on both training and test. Causes: Model too simple, insufficient features. Solutions: More complex model, feature engineering, more training.",
            "What is the bias-variance tradeoff?": "Bias: Error from wrong assumptions (underfitting). Variance: Error from sensitivity to training data (overfitting). High bias + low variance vs low bias + high variance. Goal: Balance both for optimal generalization.",
            "What is cross-validation?": "Cross-validation estimates model performance on unseen data. K-fold: Split into K parts, train on K-1, test on 1, rotate. Reduces variance of estimate. Common: 5-fold, 10-fold. Stratified for imbalanced data.",
            "What is linear regression?": "Linear regression predicts continuous target as linear combination of features. y = wx + b. Minimizes sum of squared errors. Assumptions: linearity, independence, normality of residuals, homoscedasticity.",
            "What is logistic regression?": "Logistic regression predicts probability for binary classification. Uses sigmoid function to output 0-1. Despite name, it's classification not regression. Decision boundary is linear. Interpretable coefficients.",
            "What is a decision tree?": "Decision tree makes decisions through series of if-then rules. Each node splits on a feature. Easy to interpret, handles non-linear relationships. Prone to overfitting. Basis for random forests, gradient boosting.",
            "What is k-nearest neighbors (KNN)?": "KNN classifies based on K closest training examples. No training phase (lazy learner). Distance metric (usually Euclidean). K is hyperparameter. Sensitive to scale, curse of dimensionality.",
            "What is clustering?": "Clustering groups similar data points without labels. Unsupervised learning. Types: Partition (K-means), Hierarchical, Density-based (DBSCAN). Used for segmentation, anomaly detection, data exploration.",
            "What is k-means clustering?": "K-means partitions data into K clusters. Steps: Initialize centroids, assign points to nearest, update centroids, repeat. Requires K in advance. Sensitive to initialization (use k-means++).",
            "What is the difference between classification and regression?": "Classification: Predict discrete categories (binary or multi-class). Regression: Predict continuous values. Different algorithms, metrics, loss functions. Some algorithms handle both (decision trees).",
            "What is feature engineering?": "Feature engineering creates new features from raw data to improve model performance. Includes: transformations, interactions, aggregations, encoding, domain knowledge features. Often most impactful step.",
        },
        medium: {
            "Explain the difference between L1 and L2 regularization.": "L1 (Lasso): Adds absolute value of coefficients to loss. Creates sparse models (feature selection). L2 (Ridge): Adds squared coefficients. Shrinks all coefficients. Elastic Net combines both. L1 for sparse, L2 for multicollinearity.",
            "What is gradient descent?": "Gradient descent optimizes loss function by iteratively moving in direction of steepest descent. Learning rate controls step size. Variants: Batch (all data), Stochastic (one sample), Mini-batch (subset).",
            "What is the difference between batch and stochastic gradient descent?": "Batch: Uses all data per update - stable but slow. Stochastic: One sample per update - fast, noisy, can escape local minima. Mini-batch: Best of both - typically 32-128 samples. Most common in practice.",
            "What is Random Forest?": "Random Forest: Ensemble of decision trees using bagging. Each tree trained on bootstrap sample. Random feature subset at each split. Reduces overfitting, handles non-linearity. Feature importance built-in.",
            "What is Gradient Boosting?": "Gradient Boosting builds trees sequentially, each correcting previous errors. Fits residuals. More prone to overfitting than RF but often higher accuracy. Slow to train. Popular: XGBoost, LightGBM, CatBoost.",
            "What is XGBoost and why is it popular?": "XGBoost is optimized gradient boosting. Features: Regularization, parallel processing, tree pruning, handling missing values, built-in cross-validation. Highly efficient, dominant in competitions. Hyperparameters: learning_rate, max_depth, n_estimators.",
            "What is Support Vector Machine (SVM)?": "SVM finds hyperplane maximizing margin between classes. Effective in high dimensions. Kernel trick for non-linear boundaries. Parameters: C (regularization), kernel type. Memory intensive for large datasets.",
            "What is the kernel trick in SVM?": "Kernel trick maps data to higher dimensions without explicit transformation. Common kernels: Linear, Polynomial, RBF (Gaussian). Enables non-linear decision boundaries. Computationally efficient.",
            "What is Principal Component Analysis (PCA)?": "PCA reduces dimensions while preserving variance. Finds orthogonal components ranked by variance explained. Use for: dimensionality reduction, visualization, noise reduction, multicollinearity. Assumes linear relationships.",
            "What is dimensionality reduction?": "Dimensionality reduction reduces features while preserving information. Methods: PCA, t-SNE (visualization), UMAP, Autoencoders. Benefits: Faster training, avoid overfitting, visualization. Can lose interpretability.",
            "How do you handle imbalanced datasets?": "Strategies: Resample (oversample minority, undersample majority), SMOTE (synthetic samples), class weights, different threshold, appropriate metrics (F1, AUC), collect more data.",
            "What is SMOTE?": "SMOTE (Synthetic Minority Oversampling) creates synthetic minority samples. Interpolates between existing minority points. Reduces overfitting vs simple oversampling. Variants: SMOTE-NC (categorical), Borderline-SMOTE.",
            "What is ensemble learning?": "Ensemble combines multiple models for better performance. Types: Bagging (parallel, reduce variance - RF), Boosting (sequential, reduce bias), Stacking (meta-learner). Usually outperforms single models.",
            "What is the difference between bagging and boosting?": "Bagging: Parallel training on bootstrap samples, average predictions. Reduces variance. Boosting: Sequential training, each corrects previous errors. Reduces bias. Bagging more robust, boosting often more accurate.",
            "What are precision, recall, and F1-score?": "Precision: TP/(TP+FP) - of predicted positives, how many correct. Recall: TP/(TP+FN) - of actual positives, how many found. F1: Harmonic mean of precision and recall. Trade-off between them.",
            "What is ROC-AUC curve?": "ROC plots TPR vs FPR at various thresholds. AUC = Area Under Curve. 0.5 = random, 1.0 = perfect. Threshold-independent metric. Good for comparing models. Not ideal for imbalanced data.",
            "What is confusion matrix?": "Confusion matrix shows TP, TN, FP, FN. Rows: Actual, Columns: Predicted. Derive: Accuracy, Precision, Recall, F1, Specificity. Visualizes where model makes mistakes.",
            "How do you handle missing data?": "Options: Drop rows/columns (if few), imputation (mean, median, mode, KNN, regression, MICE), indicator variable for missingness. Consider: MCAR, MAR, MNAR patterns. Tree-based models can handle missing values.",
            "What is feature selection and why is it important?": "Feature selection chooses relevant features. Methods: Filter (correlation, chi-square), Wrapper (RFE), Embedded (Lasso, tree importance). Benefits: Reduce overfitting, faster training, interpretability.",
            "What is the curse of dimensionality?": "Curse of dimensionality: As dimensions increase, data becomes sparse, distances less meaningful, more data needed. Affects: KNN, clustering. Solutions: Dimensionality reduction, feature selection.",
        },
        hard: {
            "How do you design a machine learning pipeline?": "Pipeline: Data ingestion → Cleaning → Feature engineering → Feature selection → Train/test split → Model training → Hyperparameter tuning → Evaluation → Deployment → Monitoring. Use version control, reproducibility, automation.",
            "What is hyperparameter tuning and what methods exist?": "Hyperparameters set before training. Methods: Grid Search (exhaustive), Random Search (often better), Bayesian Optimization (informed), Hyperband (early stopping). Use cross-validation. Don't tune on test set.",
            "What is GridSearchCV vs RandomizedSearchCV?": "GridSearchCV: Exhaustive search over specified parameter values. Slow for many parameters. RandomizedSearchCV: Samples random combinations. Often finds good solutions faster. Better for large search spaces.",
            "How do you handle feature importance in tree-based models?": "Methods: Mean decrease impurity (Gini/entropy), Permutation importance (shuffle and measure accuracy drop), SHAP values (game theory based). Permutation/SHAP more reliable. Consider correlation between features.",
            "What is model interpretability and why is it important?": "Interpretability: Understanding how/why model makes decisions. Important for: Trust, debugging, regulatory compliance, domain insights, bias detection. Methods: Feature importance, SHAP, LIME, partial dependence plots.",
            "What is SHAP and how does it work?": "SHAP (SHapley Additive exPlanations) attributes predictions to features using game theory. Shows feature contributions for each prediction. Consistent, locally accurate. Computationally expensive but most rigorous.",
            "What is LIME for model explanation?": "LIME (Local Interpretable Model-agnostic Explanations) explains individual predictions. Creates local linear model around prediction. Perturbs input, observes output changes. Model-agnostic but approximate.",
            "How do you deploy machine learning models?": "Options: REST API (Flask/FastAPI), containerization (Docker), cloud services (SageMaker, Vertex AI), edge deployment. Consider: Latency, throughput, versioning, monitoring, A/B testing, rollback.",
            "What is model drift and how do you handle it?": "Model drift: Performance degrades over time due to changing data patterns. Types: Data drift, concept drift. Monitor: Accuracy, feature distributions. Handle: Retrain, trigger-based updates, online learning.",
            "What is online learning?": "Online learning updates model incrementally with new data. Useful for streaming data, too large for memory, changing distributions. Algorithms: SGD-based, Vowpal Wabbit. Trade-off: Stability vs responsiveness.",
            "How do you handle high cardinality categorical variables?": "Options: Target encoding (mean target per category), frequency encoding, embedding layers (deep learning), feature hashing, grouping rare categories, leave-one-out encoding. Beware of leakage with target encoding.",
            "What is target encoding?": "Target encoding replaces category with mean of target variable. Powerful but prone to overfitting and data leakage. Solutions: Cross-validation encoding, smoothing/regularization, noise addition.",
            "How do you handle time series forecasting?": "Methods: Statistical (ARIMA, SARIMA, ETS), ML (RF, XGBoost with lag features), Deep Learning (LSTM, Transformers). Consider: Stationarity, seasonality, trend. Cross-validation: Walk-forward. Features: Lags, rolling stats.",
            "What is ARIMA model?": "ARIMA(p,d,q): AutoRegressive Integrated Moving Average. p: AR terms (lags), d: differencing (stationarity), q: MA terms (error lags). Use ACF/PACF for order selection. AIC/BIC for comparison. SARIMA for seasonality.",
            "What is the difference between batch and real-time predictions?": "Batch: Process data in bulk, scheduled (daily/hourly). Lower latency requirements. Real-time: Instant predictions, API-based. Requires low-latency infrastructure. Different architecture needs.",
        }
    },
    pythonDS: {
        easy: {
            "What is NumPy and why is it used in data science?": "NumPy provides N-dimensional arrays and mathematical operations. Faster than Python lists (vectorization, C implementation). Foundation for Pandas, Scikit-learn. Use for: Array operations, linear algebra, statistics.",
            "What is Pandas and its main data structures?": "Pandas is data manipulation library. Main structures: Series (1D labeled array), DataFrame (2D labeled table). Features: Reading data, cleaning, transformation, aggregation. Essential for data analysis.",
            "What is the difference between Series and DataFrame?": "Series: 1D, single column with index. DataFrame: 2D, multiple columns with index. DataFrame is collection of Series. Most operations return DataFrame.",
            "How do you read a CSV file in Pandas?": "pd.read_csv('file.csv'). Options: sep, header, names, dtype, parse_dates, na_values, usecols, nrows, chunksize for large files. write: df.to_csv().",
            "How do you handle missing values in Pandas?": "Detect: df.isnull(), df.isna(). Handle: df.dropna(), df.fillna(value), df.fillna(method='ffill'/'bfill'), df.interpolate(). Check: df.isnull().sum().",
            "What is Matplotlib used for?": "Matplotlib is core plotting library. Create: Line, bar, scatter, histogram, box plots. plt.figure(), plt.plot(), plt.show(). Highly customizable but verbose. Seaborn built on top.",
            "What is Seaborn and how is it different from Matplotlib?": "Seaborn: Statistical visualization library. Built on Matplotlib. Better defaults, statistical focus, works with DataFrames. Easy: heatmaps, distributions, relationships. Use Seaborn for aesthetics, Matplotlib for customization.",
            "How do you filter data in Pandas?": "Boolean indexing: df[df['col'] > 5]. Multiple conditions: df[(df['a'] > 5) & (df['b'] < 10)]. Query method: df.query('col > 5'). isin(): df[df['col'].isin([1,2,3])].",
            "What is groupby in Pandas?": "groupby splits data by column values, applies function, combines results. df.groupby('col').agg({'x': 'mean'}). Multiple: groupby(['a', 'b']). Multiple aggs: .agg({'x': ['mean', 'sum']}).",
            "How do you merge two DataFrames?": "pd.merge(df1, df2, on='key'). Parameters: how='inner/left/right/outer', left_on, right_on. Also: df1.join(df2), pd.concat([df1, df2]).",
            "What is the difference between loc and iloc?": "loc: Label-based indexing. df.loc[0:5, 'col'] - inclusive. iloc: Integer position-based. df.iloc[0:5, 0] - exclusive end. loc accepts boolean arrays.",
        },
        medium: {
            "How do you optimize Pandas operations for large datasets?": "Use: Appropriate dtypes (category, int8), chunking for reading, vectorized operations, eval() for complex expressions, avoid iterrows (use apply/vectorize), index where appropriate. Consider Dask for out-of-memory.",
            "What is vectorization in NumPy?": "Vectorization applies operations to entire arrays without loops. Much faster than Python loops (C-level). Example: arr * 2 vs for loop. Broadcasting enables operations on different shapes.",
            "How do you handle datetime data in Pandas?": "pd.to_datetime() converts strings. dt accessor: df['date'].dt.year, .month, .dayofweek. Resample: df.resample('M').mean(). Timedelta for differences. tz_localize/tz_convert for timezones.",
            "What is the apply function in Pandas?": "apply() applies function along axis. df['col'].apply(func), df.apply(func, axis=0/1). Flexible but slower than vectorized. Use for custom logic. applymap for element-wise on DataFrame.",
            "How do you reshape data using pivot tables?": "df.pivot_table(values='val', index='row', columns='col', aggfunc='mean'). pd.melt() for reverse (wide to long). stack()/unstack() for MultiIndex.",
            "What is the difference between merge, join, and concat?": "merge: SQL-style joins on columns. join: Index-based joining (faster). concat: Stack DataFrames vertically/horizontally. Use merge for flexibility, concat for simple stacking.",
            "How do you handle memory issues with large datasets?": "Optimize dtypes, read in chunks, use generators, sample for exploration, Dask/Vaex for parallel, del unused objects, gc.collect(), categorical for strings, only load needed columns.",
            "What is Scikit-learn and its main modules?": "Scikit-learn: ML library. Modules: preprocessing (scaling, encoding), model_selection (CV, GridSearch), linear_model, ensemble, cluster, metrics. Consistent API: fit(), predict(), transform().",
            "How do you create a machine learning pipeline in Scikit-learn?": "Pipeline chains steps: Pipeline([('scaler', StandardScaler()), ('model', LogisticRegression())]). Prevents data leakage, cleaner code, easier CV. ColumnTransformer for different column preprocessing.",
            "What is the difference between fit, transform, and fit_transform?": "fit(): Learn parameters from data. transform(): Apply learned parameters. fit_transform(): Both in one call (more efficient). Use fit on training data only, transform on all data.",
            "How do you perform cross-validation in Scikit-learn?": "cross_val_score(model, X, y, cv=5). cross_validate for multiple metrics. KFold, StratifiedKFold, TimeSeriesSplit. GridSearchCV combines with hyperparameter tuning.",
            "What is pickle and how is it used for saving models?": "pickle serializes Python objects. import pickle; pickle.dump(model, open('model.pkl', 'wb')). load: pickle.load(open('model.pkl', 'rb')). Alternative: joblib (better for large NumPy arrays).",
        },
        hard: {
            "How do you use Dask for parallel computing?": "Dask extends Pandas/NumPy for larger-than-memory data. Similar API: dask.dataframe, dask.array. Lazy evaluation, distributed computing. .compute() triggers execution. Use for big data on single machine or clusters.",
            "What is the difference between Pandas and PySpark?": "Pandas: Single machine, in-memory, rich functionality. PySpark: Distributed, cluster computing, lazy evaluation. Pandas for exploration, PySpark for production big data. Consider Koalas for familiar API on Spark.",
            "How do you optimize Python code for data science?": "Profile first (cProfile). Vectorize operations, use appropriate data structures, numba/cython for loops, parallel processing, efficient I/O, caching expensive computations. Avoid copying data unnecessarily.",
            "What is Cython and when would you use it?": "Cython: Python with C type declarations. Compiles to C for speed. Use for: CPU-intensive loops, extending Python with C, wrapping C libraries. Alternative: Numba for simpler cases.",
            "How do you create custom transformers in Scikit-learn?": "Inherit from BaseEstimator, TransformerMixin. Implement fit() and transform(). Use for custom preprocessing in pipelines. Ensures proper integration with cross-validation.",
            "What is feature pipeline design pattern?": "Feature pipeline: Orchestrated transformation of raw data to features. Stages: Ingestion, cleaning, feature engineering, storage. Design for: Reproducibility, reusability, online/offline consistency.",
            "How do you handle streaming data in Python?": "Libraries: Apache Kafka (confluent-kafka), Apache Spark Streaming, Faust. Patterns: Producer-consumer, windowing, stateful processing. Consider exactly-once semantics, late arrival.",
            "What is the difference between multiprocessing and multithreading?": "Threading: Shared memory, limited by GIL (CPU-bound tasks don't parallelize). Multiprocessing: Separate memory, bypasses GIL. Use threading for I/O-bound, multiprocessing for CPU-bound.",
            "How do you profile Python code for performance?": "cProfile for function-level timing. line_profiler for lines. memory_profiler for memory. snakeviz for visualization. timeit for quick tests. Profile before optimizing.",
            "What is the GIL and how does it affect data science?": "GIL (Global Interpreter Lock) prevents true parallel threading in CPython. Affects CPU-bound tasks. Solutions: Multiprocessing, NumPy (releases GIL), alternatives (PyPy), async for I/O.",
            "How do you use generators for large dataset processing?": "Generators yield items one at a time (lazy). No full list in memory. Use: Reading large files, data pipelines, infinite sequences. pd.read_csv(chunksize=n) returns generator.",
            "What is lazy evaluation and its benefits?": "Lazy evaluation delays computation until needed. Benefits: Memory efficiency, faster startup, avoids unnecessary computation. Used in: Dask, Spark, Python generators. Trade-off: Debugging harder.",
        }
    },
    sqlDS: {
        easy: {
            "What is SQL and why is it important for data science?": "SQL (Structured Query Language) queries relational databases. Most data stored in databases. Essential for: Data extraction, cleaning, transformation, aggregation. Every data scientist needs SQL.",
            "What is the difference between WHERE and HAVING?": "WHERE filters rows before grouping. HAVING filters after grouping (on aggregated values). Use WHERE for row conditions, HAVING for aggregate conditions.",
            "What are the different types of JOINs?": "INNER: Matching rows only. LEFT: All left rows + matching right. RIGHT: All right rows + matching left. FULL OUTER: All rows from both. CROSS: Cartesian product.",
            "What is GROUP BY?": "GROUP BY groups rows with same values. Used with aggregate functions (COUNT, SUM, AVG, MAX, MIN). SELECT col1, COUNT(*) FROM table GROUP BY col1.",
            "What is ORDER BY?": "ORDER BY sorts result set. ASC (default) ascending, DESC descending. Multiple columns: ORDER BY col1, col2 DESC. NULL handling varies by database.",
            "What is the difference between INNER JOIN and LEFT JOIN?": "INNER JOIN returns only matching rows from both tables. LEFT JOIN returns all left table rows with matching right table rows (NULL if no match). LEFT preserves all left data.",
            "What are aggregate functions in SQL?": "Aggregate functions operate on sets of rows: COUNT, SUM, AVG, MIN, MAX. Used with GROUP BY. COUNT(*) counts all rows, COUNT(col) counts non-NULL.",
            "What is a subquery?": "Subquery is query within query. Used in SELECT, FROM, WHERE. Scalar (returns one value), Row, Table subqueries. Can often be rewritten as JOIN for performance.",
            "What is the difference between UNION and UNION ALL?": "UNION combines results and removes duplicates (slower). UNION ALL keeps all rows including duplicates (faster). Use UNION ALL when duplicates are acceptable or impossible.",
            "How do you find duplicates in SQL?": "GROUP BY columns and HAVING COUNT(*) > 1. Example: SELECT col, COUNT(*) FROM table GROUP BY col HAVING COUNT(*) > 1. Use window functions for more control.",
        },
        medium: {
            "What are window functions in SQL?": "Window functions perform calculations across related rows without grouping. OVER(PARTITION BY... ORDER BY...). Examples: ROW_NUMBER, RANK, LAG, LEAD, SUM() OVER(). Powerful for analysis.",
            "What is the difference between ROW_NUMBER, RANK, and DENSE_RANK?": "ROW_NUMBER: Sequential (1,2,3,4). RANK: Ties get same rank, gaps after (1,2,2,4). DENSE_RANK: Ties same rank, no gaps (1,2,2,3). Choose based on tie handling needs.",
            "How do you calculate running totals in SQL?": "SUM() OVER(ORDER BY date). Running total: SUM(amount) OVER(ORDER BY date ROWS UNBOUNDED PRECEDING). Partition for groups: PARTITION BY customer_id.",
            "What is a Common Table Expression (CTE)?": "CTE is named temporary result set. WITH cte_name AS (SELECT...) SELECT * FROM cte_name. Improves readability, reusable in query, recursive CTEs for hierarchical data.",
            "How do you handle NULL values in SQL?": "IS NULL, IS NOT NULL for checks. COALESCE returns first non-NULL. NULLIF returns NULL if values equal. NULLs in aggregations usually ignored. NULL comparisons return NULL.",
            "What is COALESCE function?": "COALESCE(val1, val2, ...) returns first non-NULL value. Use for: Default values, handling NULLs. COALESCE(name, 'Unknown'). Works across databases. NVL is Oracle equivalent.",
            "How do you optimize SQL queries?": "Use indexes, avoid SELECT *, limit rows early, optimize JOINs, avoid functions on indexed columns, EXPLAIN plan, denormalize if needed, partition large tables, use CTEs appropriately.",
            "What is indexing and how does it improve performance?": "Index is data structure for fast lookups. Like book index. Speeds up WHERE, JOIN, ORDER BY. Trade-off: Faster reads, slower writes, storage space. Index frequently queried columns.",
            "What is the difference between clustered and non-clustered index?": "Clustered: Defines physical order of data. One per table. Table data = index. Non-clustered: Separate structure pointing to data. Multiple allowed. Clustered faster for range queries.",
            "How do you pivot data in SQL?": "PIVOT transforms rows to columns. CASE WHEN for simple pivots. PIVOT operator in some databases. Dynamic SQL for unknown columns. Reverse: UNPIVOT.",
        },
        hard: {
            "How do you write recursive CTEs?": "WITH RECURSIVE cte AS (anchor UNION ALL recursive). Anchor: Base case. Recursive: References itself with termination condition. Use for: Hierarchies, graphs, sequences.",
            "What is query execution plan?": "Execution plan shows how database executes query. EXPLAIN/EXPLAIN ANALYZE. Shows: Scans, joins, costs, estimates. Use to identify bottlenecks, verify index usage, optimize queries.",
            "How do you optimize joins for large tables?": "Index join columns, filter before joining, use appropriate join types, partition tables, denormalize if needed, consider hash/merge join hints, broadcast small tables in distributed systems.",
            "What is partitioning in databases?": "Partitioning divides large tables into smaller pieces. Types: Range (dates), Hash, List. Benefits: Faster queries (partition pruning), easier maintenance, parallelism. Use for large tables.",
            "How do you handle slowly changing dimensions?": "SCD tracks historical changes. Type 1: Overwrite. Type 2: New row with timestamps/flags. Type 3: Previous value column. Type 2 most common for history. Design based on reporting needs.",
            "What is the difference between OLTP and OLAP?": "OLTP: Online Transaction Processing - normalized, fast writes, current data, operations. OLAP: Online Analytical Processing - denormalized, complex queries, historical, analytics. Different optimization strategies.",
            "How do you design a data warehouse schema?": "Star schema: Central fact table, dimension tables. Snowflake: Normalized dimensions. Design: Identify facts (measures) and dimensions, define granularity, establish relationships, plan for growth.",
            "What is star schema vs snowflake schema?": "Star: Fact table + denormalized dimension tables. Simple, fast queries. Snowflake: Normalized dimensions (more tables). Less redundancy, more joins. Star preferred for query performance.",
            "How do you handle time-series data in SQL?": "Index on time column, partition by time, window functions for calculations, LAG/LEAD for previous/next values, gap detection, aggregation by time periods, proper date/time types.",
            "What is database normalization and denormalization?": "Normalization: Reduce redundancy through decomposition (1NF through 5NF). Denormalization: Add redundancy for query performance. Normalize for OLTP, denormalize for OLAP. Balance based on use case.",
        }
    },
    deepLearning: {
        easy: {
            "What is deep learning?": "Deep learning is ML using neural networks with multiple layers. Learns hierarchical representations. Excels at: Images, text, speech. Requires: Large data, compute. Subset of ML, which is subset of AI.",
            "What is a neural network?": "Neural network is interconnected layers of nodes (neurons). Input layer receives data, hidden layers process, output layer gives result. Each connection has weight. Learn by adjusting weights.",
            "What is an activation function?": "Activation function introduces non-linearity. Without it, network is just linear transformation. Common: ReLU (most used), Sigmoid (0-1, vanishing gradient), Tanh (-1 to 1), Softmax (multiclass output).",
            "What is the difference between sigmoid, tanh, and ReLU?": "Sigmoid: 0-1, vanishing gradient problem. Tanh: -1 to 1, zero-centered, still vanishing gradient. ReLU: max(0,x), fast, most popular, dying ReLU problem. Leaky ReLU addresses dying neurons.",
            "What is backpropagation?": "Backpropagation calculates gradients of loss w.r.t. weights. Uses chain rule. Gradients flow backward from output. Weights updated via gradient descent. Foundation of neural network training.",
            "What is an epoch, batch, and iteration?": "Epoch: One complete pass through training data. Batch: Subset of data for one forward/backward pass. Iteration: One batch update. Epochs = Iterations × Batch Size / Dataset Size.",
            "What is the difference between deep learning and machine learning?": "ML: Broader field, handcrafted features, various algorithms. DL: Subset using deep neural networks, automatic feature learning, needs more data/compute. DL excels at unstructured data.",
            "What is TensorFlow?": "TensorFlow is Google's deep learning framework. Features: Computation graphs, GPU support, TensorFlow Serving for deployment, TensorBoard for visualization. Keras is high-level API.",
            "What is PyTorch?": "PyTorch is Facebook's deep learning framework. Dynamic computation graphs, Pythonic, research-friendly. Features: Autograd, TorchScript for production. Gaining popularity over TensorFlow.",
            "What is a loss function?": "Loss function measures prediction error. Model minimizes it during training. Regression: MSE, MAE. Classification: Cross-entropy, Hinge loss. Choice depends on task and data.",
        },
        medium: {
            "What is a Convolutional Neural Network (CNN)?": "CNN specializes in spatial data (images). Convolutional layers detect features, pooling reduces dimensions. Architecture: Conv → Pool → Conv → Pool → FC. Learns hierarchical features.",
            "What is a Recurrent Neural Network (RNN)?": "RNN processes sequential data. Hidden state carries information across time steps. Issues: Vanishing/exploding gradients for long sequences. Use for: Text, time series, speech.",
            "What is LSTM and why is it used?": "LSTM (Long Short-Term Memory) solves vanishing gradient in RNNs. Gates control information flow: Forget, Input, Output gates. Cell state carries long-term memory. Standard for sequential data.",
            "What is dropout and why is it used?": "Dropout randomly deactivates neurons during training. Prevents co-adaptation, reduces overfitting. Typically 0.2-0.5. Applied between layers. Disabled during inference.",
            "What is batch normalization?": "Batch norm normalizes layer inputs to zero mean, unit variance. Benefits: Faster training, regularization, less sensitive to initialization, allows higher learning rates. Applied before activation.",
            "What is transfer learning?": "Transfer learning uses pre-trained model for new task. Fine-tune: Adapt layers, freeze others. Benefits: Less data needed, faster training. Common in vision (ImageNet), NLP (BERT).",
            "What is the vanishing gradient problem?": "Vanishing gradient: Gradients become tiny in deep networks, early layers don't learn. Caused by: Sigmoid/Tanh (squash gradients), many layers. Solutions: ReLU, skip connections, LSTM, batch norm.",
            "What are optimizers like Adam, SGD, RMSprop?": "SGD: Basic gradient descent, may oscillate. Momentum: Accelerates SGD. RMSprop: Adaptive learning rate per parameter. Adam: Combines momentum + RMSprop. Adam most popular default.",
            "What is learning rate and how do you choose it?": "Learning rate controls step size in optimization. Too high: Diverge. Too low: Slow, stuck in local minima. Find: Learning rate finder, start 1e-3. Schedulers: StepLR, ReduceLROnPlateau, warmup.",
            "What is data augmentation?": "Data augmentation artificially expands training data. Images: Flip, rotate, crop, color jitter. Text: Synonym replacement, back-translation. Reduces overfitting, improves generalization.",
        },
        hard: {
            "What is the architecture of ResNet?": "ResNet uses skip connections (residual connections). Input added to output: y = F(x) + x. Enables very deep networks (50-152+ layers). Solves vanishing gradient. Foundation for modern architectures.",
            "What is attention mechanism?": "Attention weighs importance of different input parts. Self-attention: Within sequence. Cross-attention: Between sequences. Enables focusing on relevant information. Core of Transformers.",
            "What are Transformers and how do they work?": "Transformers use self-attention for sequence modeling. No recurrence, fully parallel. Architecture: Encoder-Decoder or Encoder-only/Decoder-only. Revolutionized NLP. Basis for BERT, GPT.",
            "What is BERT and how is it used?": "BERT: Bidirectional Encoder Representations from Transformers. Pre-trained on masked LM + next sentence prediction. Fine-tune for: Classification, NER, QA. Bidirectional context understanding.",
            "What is GPT architecture?": "GPT: Generative Pre-trained Transformer. Decoder-only, autoregressive. Pre-trained on next token prediction. Can generate text. GPT-3/4 show emergent capabilities at scale.",
            "How do you handle overfitting in deep learning?": "Regularization (dropout, weight decay), data augmentation, early stopping, simpler architecture, batch normalization, ensemble, more data, cross-validation for hyperparameters.",
            "What is model pruning and quantization?": "Pruning removes unnecessary weights (magnitude-based, structured). Quantization reduces precision (FP32 to INT8). Both reduce size and speed up inference. Used for edge deployment.",
            "How do you deploy deep learning models in production?": "Export (ONNX, TorchScript, SavedModel). Serve: TensorFlow Serving, TorchServe, Triton. Package: Docker, Kubernetes. Optimize: TensorRT, OpenVINO. Monitor latency, accuracy, drift.",
            "What is federated learning?": "Federated learning trains on decentralized data. Model goes to data, not vice versa. Privacy-preserving. Challenges: Communication, heterogeneous data, security. Used in mobile keyboards, healthcare.",
            "What are GANs and how do they work?": "GANs: Generator + Discriminator in adversarial training. Generator creates fake data, Discriminator distinguishes real/fake. Both improve through competition. Applications: Image generation, super-resolution, style transfer.",
        }
    },
    dataEngineering: {
        easy: {
            "What is ETL?": "ETL: Extract (from sources), Transform (clean, aggregate, format), Load (to destination). Foundation of data pipelines. ELT variant loads first, transforms in warehouse. Essential for data integration.",
            "What is the difference between data lake and data warehouse?": "Data warehouse: Structured, schema-on-write, optimized for BI. Data lake: All data types, schema-on-read, raw storage. Warehouse for analytics, lake for flexibility. Lakehouse combines both.",
            "What is Apache Spark?": "Spark is distributed computing framework. Faster than MapReduce (in-memory). Components: SQL, Streaming, MLlib, GraphX. Use for: Large-scale processing, ML, ETL. PySpark for Python.",
            "What is Hadoop?": "Hadoop is distributed storage and processing framework. Components: HDFS (storage), MapReduce (processing), YARN (resource management). Foundation for big data ecosystem. Spark often used on top.",
            "What is the difference between batch and stream processing?": "Batch: Process accumulated data at intervals. Higher throughput, latency acceptable. Stream: Process data as it arrives. Real-time, lower latency. Choose based on latency requirements.",
        },
        medium: {
            "How does MapReduce work?": "MapReduce: Map phase processes data in parallel, produces key-value pairs. Shuffle sorts and groups by key. Reduce aggregates values per key. Fault-tolerant, scalable. Superseded by Spark.",
            "What is Apache Kafka?": "Kafka is distributed event streaming platform. Producers publish to topics, consumers subscribe. Partitions for parallelism. Use for: Real-time pipelines, messaging, log aggregation. High throughput.",
            "How do you handle data quality issues?": "Validate at ingestion, implement data contracts, monitor distributions, set up alerts, data profiling, standardize formats, handle nulls consistently, document lineage, data quality dashboards.",
            "What is data pipeline architecture?": "Pipeline architecture: Source systems → Ingestion → Storage → Processing → Serving. Considerations: Reliability, scalability, monitoring, orchestration (Airflow), idempotency, error handling.",
            "What is the difference between Spark RDD and DataFrame?": "RDD: Low-level, functional operations, no optimization. DataFrame: Higher-level, SQL-like, Catalyst optimizer, schema. DataFrame preferred for most tasks. Dataset adds type safety (Scala/Java).",
        },
        hard: {
            "How do you design a real-time data pipeline?": "Components: Message queue (Kafka), stream processor (Spark Streaming, Flink), storage (database, data lake). Consider: Exactly-once semantics, late data handling, watermarks, state management.",
            "What is exactly-once processing in streaming?": "Exactly-once ensures each record processed exactly once despite failures. Achieved through: Checkpointing, idempotent writes, transactional producers/consumers. Flink provides true exactly-once.",
            "How do you handle late-arriving data?": "Use watermarks to track event time progress. Allowed lateness windows. Late triggers for delayed updates. Handle in processing logic. Trade-off between completeness and latency.",
            "What is Delta Lake?": "Delta Lake adds reliability to data lakes. Features: ACID transactions, schema enforcement, time travel (versioning), upserts. Open-source, works with Spark. Enables lakehouse architecture.",
            "How do you optimize Spark jobs?": "Partitioning (reduce shuffles), caching (persist frequently used), broadcast joins (small tables), avoid UDFs (use built-in), tune executor memory/cores, use DataFrames over RDDs, coalesce for fewer partitions.",
        }
    }
};
